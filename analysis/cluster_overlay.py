import pandas as pd
import numpy as np
from scipy.stats import ttest_ind as t, norm
from itertools import combinations

'''
overlays clusters (generated by model) on top of lineup data

generates statistical significance of every cluster combination

'''



######################################
#read in the yearly lineup data that should be merged for analysis
raw_lineup_data_2016_17 = pd.read_csv('~/capstone_project/data/lineup_data_2016_17.csv')
raw_lineup_data_2015_16 = pd.read_csv('~/capstone_project/data/lineup_data_2015_16.csv')
raw_lineup_data_2014_15 = pd.read_csv('~/capstone_project/data/lineup_data_2014_15.csv')
raw_lineup_data_2013_14 = pd.read_csv('~/capstone_project/data/lineup_data_2013_14.csv')
raw_lineup_data_2012_13 = pd.read_csv('~/capstone_project/data/lineup_data_2012_13.csv')
# raw_lineup_data_2012_13 = pd.read_csv('~/capstone_project/data/lineup_data_2012_13.csv')

raw_lineup_data_dfs = [raw_lineup_data_2016_17,raw_lineup_data_2015_16, raw_lineup_data_2014_15,raw_lineup_data_2013_14,raw_lineup_data_2012_13]

#read in the player clusters (model output)
player_clusters = pd.read_csv('~/capstone_project/data/t_sne_clusters.csv')
#####################################



def add_clusters_to_lineups(raw_lineup_data_dfs, clusters_df):
    """
    INPUT:
    1) a list of dataframes of lineup data by season (ie one or more seasons)
    2) a dataframe of cluster assignments per player_id

    OUTPUT:
    1) a dataframe of every lineup transformed into cluster combinations

    """
    #convert the lineup ids to sorted list of ints
    lineups_df = raw_lineup_data_dfs[0]
    for i, df in enumerate(raw_lineup_data_dfs):
        if i > 0:
            lineups_df = lineups_df.append(df, ignore_index= True)

    lineup_ids = []
    for i in xrange(lineups_df.shape[0]):
        lineup = lineups_df['lineup_ids'].iloc[i].replace(' ','').split('-')
        lineup = [int(x) for x in lineup]
        lineup.sort()
        lineup_ids.append(lineup)
    lineups_df['lineup_ids'] = lineup_ids

    #create list of players that have cluster assignments
    players_with_cluster  = clusters_df.player_id.tolist()

    #create list to hold lists of clusters (later append this to df as a Series)
    cluster_lst = []
    #iterate over each line in lineup_data
    for i in xrange(lineups_df.shape[0]):
        #index into the linup_ids attribute
        lineup = lineups_df['lineup_ids'].iloc[i]

        #create empty temp list to hold clusters for each row
        clusters_temp = []

        #iterate over the lineup_ids list
        for player_id in lineup:
            print player_id
            if player_id in players_with_cluster:
                #for each id in the list to pull the corresponding cluster from player_clusters
                player_cluster = clusters_df['cluster'][clusters_df['player_id'] == player_id]
                clusters_temp.append(int(player_cluster.iloc[0]))
            else:
                clusters_temp.append('X')
        #add the cluster to the temp list
        clusters_temp.sort()
        clusters_temp = [str(x) for x in clusters_temp]
        #add the list to the list of clusters
        cluster_lst.append(clusters_temp)
    #add the whole list of clusters to the df
    lineups_df['clusters'] = cluster_lst
    return lineups_df

def get_stat_significance(cluster_lineup_df, lineup_minute_min):
    #takes in file/df of all cluster combinations
    clusters_lineups = cluster_lineup_df.copy()

    #convert clusters to str so they can be grouped, drop unnecessary columns
    #drop any cluster combos containing players that aren't being included in analyis (ie low minutes played)
    #group by lineup ids (to aggregate lineup stats over multiple seasons)
    #create net rating column and net per min
    clusters_str = clusters_lineups['clusters'].apply(lambda x: ','.join(x))
    clusters_lineups['lineup_ids'] = clusters_lineups['lineup_ids'].apply(lambda x: map(str, x))
    lineup_ids_str = clusters_lineups['lineup_ids'].apply(lambda x: ','.join(x))
    clusters_lineups['clusters'] = clusters_str
    clusters_lineups['lineup_ids'] = lineup_ids_str
    clusters_lineups = clusters_lineups[['lineup_ids','lineup_names','clusters','points_scored','points_allowed','MIN_TOT']]
    clusters_lineups = clusters_lineups[clusters_lineups.clusters.str.contains("X") == False]
    # clusters_lineups['lineup_ids'] = clusters_lineups['lineup_ids'].apply(lambda x: map(int, x))
    clusters_lineups = clusters_lineups.groupby(['lineup_ids', 'clusters','lineup_names']).sum()
    clusters_lineups.reset_index(inplace=True)
    clusters_lineups = clusters_lineups[clusters_lineups.MIN_TOT >= lineup_minute_min]

    #create set of all unique cluster combos
    unique_cluster_combos = sorted(list(set(clusters_lineups['clusters'].tolist())))
    clusters_lineups['net'] = clusters_lineups['points_scored'] - clusters_lineups['points_allowed']
    clusters_lineups['net_per_min'] = clusters_lineups['net']/clusters_lineups['MIN_TOT']
    net_min_cluster_combo_all = clusters_lineups['net_per_min'].tolist()

    aggregated_cluster_combos = clusters_lineups[['clusters','net','MIN_TOT']].groupby('clusters').sum()
    aggregated_cluster_combos['net_per_min'] = aggregated_cluster_combos['net']/aggregated_cluster_combos['MIN_TOT']
    aggregated_cluster_combos.reset_index(inplace = True)
    aggregated_cluster_combos.sort('clusters', inplace = True)
    unique_c_combos_net_min = aggregated_cluster_combos['net_per_min'].tolist()

    #create an array over the df of the net_plus_minus/min for each lineup


    #iterate over the set, index into the df for that cluster combo (eg filter out everything else)
    # create an array representing the net_plus_minus/min for that cluster combo
    # run a welsh's t-test for each using the cluster combo array and the overall array
    #store scores

    lst_of_dicts = []
    for cluster, net in zip(unique_cluster_combos, unique_c_combos_net_min):
        net_min_cluster_combo = clusters_lineups['net_per_min'][clusters_lineups.clusters == cluster].tolist()
        cluster_combo_min = clusters_lineups[['clusters','MIN_TOT']].groupby('clusters').sum()
        cluster_combo_min.reset_index(inplace = True)
        cluster_combo_min = cluster_combo_min['MIN_TOT'][cluster_combo_min['clusters'] == cluster].sum()
        t_score, p_val = t(net_min_cluster_combo_all,net_min_cluster_combo, equal_var = False)
        temp_dict = {'cluster_combo':cluster,'min':cluster_combo_min,'t_score':t_score, 'p_val':round(p_val,5), 'net_per_min':net}
        lst_of_dicts.append(temp_dict)

    cluster_combo_scores = pd.DataFrame(lst_of_dicts)
    cluster_combo_scores = cluster_combo_scores.sort('net_per_min')
    cluster_combo_scores = cluster_combo_scores[cluster_combo_scores['p_val'] <= .1]
    cluster_combo_scores = cluster_combo_scores[cluster_combo_scores['min'] >= 75]
    cluster_combo_scores.sort('net_per_min',inplace = True, ascending = False )
    cluster_combo_scores.drop('min', inplace = True, axis = 1)

    aggregated_cluster_combos = aggregated_cluster_combos.sort('net_per_min', ascending = False)
    aggregated_cluster_combos = aggregated_cluster_combos[aggregated_cluster_combos.MIN_TOT >= 60]

    clusters_lineups.to_csv('~/capstone_project/data/clusters_lineups.csv')

    return aggregated_cluster_combos, clusters_lineups, cluster_combo_scores


def get_three_combo(clusters_lineups_df):
    clusters_lineups = clusters_lineups_df

    #get list of cluster combos
    clusters = [x for x in range(10)]
    combos = combinations(clusters, 3)
    combos = map(list, combos)
    combos = sorted(list(combos))

    #add clusters as list of ints
    lineups_int = []
    for i in xrange(clusters_lineups_df.shape[0]):
        lineup = clusters_lineups_df['clusters'].iloc[i]
        lineup = lineup.split(',')
        lineup = [int(s) for s in lineup]
        lineups_int.append(lineup)
    clusters_lineups_df['lineups_i'] = pd.Series(lineups_int)

    combo_3_lst_of_lsts = []
    for i in xrange(clusters_lineups_df.shape[0]):
        clusts = clusters_lineups_df['lineups_i'].iloc[i]
        combo_3_lst = combinations(clusts, 3)
        combo_3_lst = map(list, combo_3_lst)
        combo_3_lst = sorted(list(combo_3_lst))
        combo_3_lst_of_lsts.append(combo_3_lst)
    clusters_lineups_df['combo_3_list'] = pd.Series(combo_3_lst_of_lsts)


    lst_of_dicts = []
    for c in combos:
        for i in xrange(clusters_lineups_df.shape[0]):
            if c in clusters_lineups_df['combo_3_list'].iloc[i]:
                temp_dict = {'combo':str(c),
                            'all_clusters': clusters_lineups_df['clusters'].iloc[i],
                            'points_allowed':clusters_lineups_df['points_allowed'].iloc[i].sum(),
                            'points_scored':clusters_lineups_df['points_scored'].iloc[i].sum(),
                            'min_tot':clusters_lineups_df['MIN_TOT'].iloc[i].sum(),
                            'net':clusters_lineups_df['net'].iloc[i].sum(),
                            'net_per_min':clusters_lineups_df['net_per_min'].iloc[i].sum()
                            }
                lst_of_dicts.append(temp_dict)


    combos_3_df = pd.DataFrame(lst_of_dicts)

    aggregated_combos_3 = combos_3_df[['combo','net','min_tot']].groupby('combo').sum()
    aggregated_combos_3['net_per_min'] = aggregated_combos_3['net']/aggregated_combos_3['min_tot']
    aggregated_combos_3.reset_index(inplace = True)
    aggregated_combos_3.sort('combo', inplace = True)
    unique_combos_3_net_min = aggregated_combos_3['net_per_min'].tolist()

    net_min_c3_combo_all = combos_3_df['net_per_min'].tolist()

    combos = map(str, combos)

    # lst_of_dicts = []
    # for c3, net in zip(combos, unique_combos_3_net_min):
    #     net_min_c3_combo = combos_3_df['net_per_min'][combos_3_df.combo == c3].tolist()
    #     print net_min_c3_combo
    #     c3_combo_min = combos_3_df[['combo','min_tot']].groupby('combo').sum()
    #     c3_combo_min.reset_index(inplace = True)
    #     c3_combo_min = c3_combo_min['min_tot'][c3_combo_min['combo'] == c3].sum()

    #
    #     t_score, p_val = t(net_min_c3_combo_all,net_min_c3_combo, equal_var = True)
    #
    #     temp_dict = {'c3_combo':c3,'min':c3_combo_min,'t_score':t_score, 'p_val':round(p_val,5), 'net_per_min':net}
    #     lst_of_dicts.append(temp_dict)
    #
    # c3_combo_scores = pd.DataFrame(lst_of_dicts)
    # c3_combo_scores = c3_combo_scores.sort('net_per_min')
    # c3_combo_scores = c3_combo_scores[c3_combo_scores['p_val'] <= .1]
    # # c3_combo_scores = c3_combo_scores[c3_combo_scores['min'] >= 75]
    # c3_combo_scores.sort('net_per_min',inplace = True, ascending = False )
    # c3_combo_scores.drop('min', inplace = True, axis = 1)




    total_min_net = combos_3_df[['combo', 'net','min_tot']].groupby('combo').sum()
    total_net = float(total_min_net['net'].sum())
    total_min = float(total_min_net['min_tot'].sum())
    total_net_min = total_net/total_min
    total_row = combos_3_df.shape[0]


    lst_of_dicts = []
    for c3 in combos:

        net_min_combo = float(aggregated_combos_3['net_per_min'][aggregated_combos_3.combo == c3])
        print net_min_combo
        total_min_combo = float(aggregated_combos_3['min_tot'][aggregated_combos_3.combo == c3])
        print total_min_combo
        row_combo = combos_3_df[combos_3_df.combo == c3].shape[0]
        print row_combo

        combos_min_limit = combos_3_df[combos_3_df.min_tot > 0]
        std_c3 = np.std(np.array(aggregated_combos_3['net_per_min']))
        print std_c3

        z_score = (net_min_combo - 0)/std_c3
        print z_score

        p_val = norm.sf(abs(z_score))*2
        print p_val

        temp_dict = {'c3_combo':c3,'p_val':round(p_val,5),'z_score':z_score,'net_per_min':net_min_combo,'total_min_combo': total_min_combo,'observations':row_combo}
        lst_of_dicts.append(temp_dict)

    c3_combo_scores = pd.DataFrame(lst_of_dicts)
    c3_combo_scores = c3_combo_scores.sort('net_per_min')
    c3_combo_scores = c3_combo_scores[c3_combo_scores['p_val'] <= .1]
    c3_combo_scores.sort('net_per_min',inplace = True, ascending = False )
    # c3_combo_scores.drop('min_tot', inplace = True, axis = 1)


    return total_net_min, lst_of_dicts, c3_combo_scores, aggregated_combos_3, combos_3_df



if __name__ == '__main__':
    # lineup_cluster_df = add_clusters_to_lineups(raw_lineup_data_dfs,player_clusters)
    # aggregated_cluster_combos, clusters_lineups, cluster_combo_scores = get_stat_significance(lineup_cluster_df,15)
    clusters_lineups = pd.read_csv('~/capstone_project/data/clusters_lineups.csv')
    net_min, lst, test, test2, test3 = get_three_combo(clusters_lineups)
